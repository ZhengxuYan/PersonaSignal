### repo: /Users/jasonyan/Desktop/PersonaSignal/.venv/src/tinker-cookbook @ 0adfc25b7e1ea99d3ec4aab311516fab8c5e5120
modules: tinker_cookbook
-- repo-wide (vs HEAD, staged+unstaged) --
diff --git a/tinker_cookbook/tokenizer_utils.py b/tinker_cookbook/tokenizer_utils.py
index 499c1c1..4c703f6 100644
--- a/tinker_cookbook/tokenizer_utils.py
+++ b/tinker_cookbook/tokenizer_utils.py
@@ -25,7 +25,7 @@ def get_tokenizer(model_name: str) -> Tokenizer:
     from transformers.models.auto.tokenization_auto import AutoTokenizer
 
     # Avoid gating of Llama 3 models:
-    if model_name.startswith("meta-llama/Llama-3"):
-        model_name = "baseten/Meta-Llama-3-tokenizer"
+    # if model_name.startswith("meta-llama/Llama-3"):
+    #     model_name = "baseten/Meta-Llama-3-tokenizer"
 
     return AutoTokenizer.from_pretrained(model_name, use_fast=True)
